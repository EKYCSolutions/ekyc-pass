Metadata-Version: 1.2
Name: yolox
Version: 0.1.0
Summary: UNKNOWN
Home-page: UNKNOWN
Author: basedet team
License: UNKNOWN
Description: <<<<<<< HEAD
        # ByteTrack
        
        [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bytetrack-multi-object-tracking-by-1/multi-object-tracking-on-mot17)](https://paperswithcode.com/sota/multi-object-tracking-on-mot17?p=bytetrack-multi-object-tracking-by-1)
        
        [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bytetrack-multi-object-tracking-by-1/multi-object-tracking-on-mot20-1)](https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=bytetrack-multi-object-tracking-by-1)
        
        #### ByteTrack is a simple, fast and strong multi-object tracker.
        
        <p align="center"><img src="assets/sota.png" width="500"/></p>
        
        > [**ByteTrack: Multi-Object Tracking by Associating Every Detection Box**](https://arxiv.org/abs/2110.06864)
        > 
        > Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, Xinggang Wang
        > 
        > *[arXiv 2110.06864](https://arxiv.org/abs/2110.06864)*
        
        ## Demo Links
        | Google Colab Demo | Huggingface Demo |                  YouTube Tutorial                   | Original Paper: ByteTrack |
        |:-----------------:|:----------------:|:---------------------------------------------------:|:-------------------------:|
        |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bDilg4cmXFa8HCKHbsZ_p16p0vrhLyu0?usp=sharing)|[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/bytetrack)|[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/QCG8QMhga9k)|[arXiv 2110.06864](https://arxiv.org/abs/2110.06864) |
        * Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio).
        
        
        ## Abstract
        Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 scores ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU.
        <p align="center"><img src="assets/teasing.png" width="400"/></p>
        
        ## News
        * (2022.07) Our paper is accepted by ECCV 2022!
        * (2022.06) A [nice re-implementation](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/mot/bytetrack) by Baidu [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)!
        
        ## Tracking performance
        ### Results on MOT challenge test set
        | Dataset    |  MOTA | IDF1 | HOTA | MT | ML | FP | FN | IDs | FPS |
        |------------|-------|------|------|-------|-------|------|------|------|------|
        |MOT17       | 80.3 | 77.3 | 63.1 | 53.2% | 14.5% | 25491 | 83721 | 2196 | 29.6 |
        |MOT20       | 77.8 | 75.2 | 61.3 | 69.2% | 9.5%  | 26249 | 87594 | 1223 | 13.7 |
        
        ### Visualization results on MOT challenge test set
        <img src="assets/MOT17-01-SDP.gif" width="400"/>   <img src="assets/MOT17-07-SDP.gif" width="400"/>
        <img src="assets/MOT20-07.gif" width="400"/>   <img src="assets/MOT20-08.gif" width="400"/>
        
        ## Installation
        ### 1. Installing on the host machine
        Step1. Install ByteTrack.
        ```shell
        git clone https://github.com/ifzhang/ByteTrack.git
        cd ByteTrack
        pip3 install -r requirements.txt
        python3 setup.py develop
        ```
        
        Step2. Install [pycocotools](https://github.com/cocodataset/cocoapi).
        
        ```shell
        pip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
        ```
        
        Step3. Others
        ```shell
        pip3 install cython_bbox
        ```
        ### 2. Docker build
        ```shell
        docker build -t bytetrack:latest .
        
        # Startup sample
        mkdir -p pretrained && \
        mkdir -p YOLOX_outputs && \
        xhost +local: && \
        docker run --gpus all -it --rm \
        -v $PWD/pretrained:/workspace/ByteTrack/pretrained \
        -v $PWD/datasets:/workspace/ByteTrack/datasets \
        -v $PWD/YOLOX_outputs:/workspace/ByteTrack/YOLOX_outputs \
        -v /tmp/.X11-unix/:/tmp/.X11-unix:rw \
        --device /dev/video0:/dev/video0:mwr \
        --net=host \
        -e XDG_RUNTIME_DIR=$XDG_RUNTIME_DIR \
        -e DISPLAY=$DISPLAY \
        --privileged \
        bytetrack:latest
        ```
        
        ## Data preparation
        
        Download [MOT17](https://motchallenge.net/), [MOT20](https://motchallenge.net/), [CrowdHuman](https://www.crowdhuman.org/), [Cityperson](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md), [ETHZ](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md) and put them under <ByteTrack_HOME>/datasets in the following structure:
        ```
        datasets
           |——————mot
           |        └——————train
           |        └——————test
           └——————crowdhuman
           |         └——————Crowdhuman_train
           |         └——————Crowdhuman_val
           |         └——————annotation_train.odgt
           |         └——————annotation_val.odgt
           └——————MOT20
           |        └——————train
           |        └——————test
           └——————Cityscapes
           |        └——————images
           |        └——————labels_with_ids
           └——————ETHZ
                    └——————eth01
                    └——————...
                    └——————eth07
        ```
        
        Then, you need to turn the datasets to COCO format and mix different training data:
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/convert_mot17_to_coco.py
        python3 tools/convert_mot20_to_coco.py
        python3 tools/convert_crowdhuman_to_coco.py
        python3 tools/convert_cityperson_to_coco.py
        python3 tools/convert_ethz_to_coco.py
        ```
        
        Before mixing different datasets, you need to follow the operations in [mix_xxx.py](https://github.com/ifzhang/ByteTrack/blob/c116dfc746f9ebe07d419caa8acba9b3acfa79a6/tools/mix_data_ablation.py#L6) to create a data folder and link. Finally, you can mix the training data:
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/mix_data_ablation.py
        python3 tools/mix_data_test_mot17.py
        python3 tools/mix_data_test_mot20.py
        ```
        
        
        ## Model zoo
        
        ### Ablation model
        
        Train on CrowdHuman and MOT17 half train, evaluate on MOT17 half val
        
        | Model    |  MOTA | IDF1 | IDs | FPS |
        |------------|-------|------|------|------|
        |ByteTrack_ablation [[google]](https://drive.google.com/file/d/1iqhM-6V_r1FpOlOzrdP_Ejshgk0DxOob/view?usp=sharing), [[baidu(code:eeo8)]](https://pan.baidu.com/s/1W5eRBnxc4x9V8gm7dgdEYg) | 76.6 | 79.3 | 159 | 29.6 |
        
        ### MOT17 test model
        
        Train on CrowdHuman, MOT17, Cityperson and ETHZ, evaluate on MOT17 train.
        
        * **Standard models**
        
        | Model    |  MOTA | IDF1 | IDs | FPS |
        |------------|-------|------|------|------|
        |bytetrack_x_mot17 [[google]](https://drive.google.com/file/d/1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5/view?usp=sharing), [[baidu(code:ic0i)]](https://pan.baidu.com/s/1OJKrcQa_JP9zofC6ZtGBpw) | 90.0 | 83.3 | 422 | 29.6 |
        |bytetrack_l_mot17 [[google]](https://drive.google.com/file/d/1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz/view?usp=sharing), [[baidu(code:1cml)]](https://pan.baidu.com/s/1242adimKM6TYdeLU2qnuRA) | 88.7 | 80.7 | 460 | 43.7 |
        |bytetrack_m_mot17 [[google]](https://drive.google.com/file/d/11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun/view?usp=sharing), [[baidu(code:u3m4)]](https://pan.baidu.com/s/1fKemO1uZfvNSLzJfURO4TQ) | 87.0 | 80.1 | 477 | 54.1 |
        |bytetrack_s_mot17 [[google]](https://drive.google.com/file/d/1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj/view?usp=sharing), [[baidu(code:qflm)]](https://pan.baidu.com/s/1PiP1kQfgxAIrnGUbFP6Wfg) | 79.2 | 74.3 | 533 | 64.5 |
        
        * **Light models**
        
        | Model    |  MOTA | IDF1 | IDs | Params(M) | FLOPs(G) |
        |------------|-------|------|------|------|-------|
        |bytetrack_nano_mot17 [[google]](https://drive.google.com/file/d/1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX/view?usp=sharing), [[baidu(code:1ub8)]](https://pan.baidu.com/s/1dMxqBPP7lFNRZ3kFgDmWdw) | 69.0 | 66.3 | 531 | 0.90 | 3.99 |
        |bytetrack_tiny_mot17 [[google]](https://drive.google.com/file/d/1LFAl14sql2Q5Y9aNFsX_OqsnIzUD_1ju/view?usp=sharing), [[baidu(code:cr8i)]](https://pan.baidu.com/s/1jgIqisPSDw98HJh8hqhM5w) | 77.1 | 71.5 | 519 | 5.03 | 24.45 |
        
        
        
        ### MOT20 test model
        
        Train on CrowdHuman and MOT20, evaluate on MOT20 train.
        
        
        | Model    |  MOTA | IDF1 | IDs | FPS |
        |------------|-------|------|------|------|
        |bytetrack_x_mot20 [[google]](https://drive.google.com/file/d/1HX2_JpMOjOIj1Z9rJjoet9XNy_cCAs5U/view?usp=sharing), [[baidu(code:3apd)]](https://pan.baidu.com/s/1bowJJj0bAnbhEQ3_6_Am0A) | 93.4 | 89.3 | 1057 | 17.5 |
        
        
        ## Training
        
        The COCO pretrained YOLOX model can be downloaded from their [model zoo](https://github.com/Megvii-BaseDetection/YOLOX/tree/0.1.0). After downloading the pretrained models, you can put them under <ByteTrack_HOME>/pretrained.
        
        * **Train ablation model (MOT17 half train and CrowdHuman)**
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/train.py -f exps/example/mot/yolox_x_ablation.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth
        ```
        
        * **Train MOT17 test model (MOT17 train, CrowdHuman, Cityperson and ETHZ)**
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/train.py -f exps/example/mot/yolox_x_mix_det.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth
        ```
        
        * **Train MOT20 test model (MOT20 train, CrowdHuman)**
        
        For MOT20, you need to clip the bounding boxes inside the image.
        
        Add clip operation in [line 134-135 in data_augment.py](https://github.com/ifzhang/ByteTrack/blob/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/data_augment.py#L134), [line 122-125 in mosaicdetection.py](https://github.com/ifzhang/ByteTrack/blob/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/datasets/mosaicdetection.py#L122), [line 217-225 in mosaicdetection.py](https://github.com/ifzhang/ByteTrack/blob/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/datasets/mosaicdetection.py#L217), [line 115-118 in boxes.py](https://github.com/ifzhang/ByteTrack/blob/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/utils/boxes.py#L115).
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/train.py -f exps/example/mot/yolox_x_mix_mot20_ch.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth
        ```
        
        * **Train custom dataset**
        
        First, you need to prepare your dataset in COCO format. You can refer to [MOT-to-COCO](https://github.com/ifzhang/ByteTrack/blob/main/tools/convert_mot17_to_coco.py) or [CrowdHuman-to-COCO](https://github.com/ifzhang/ByteTrack/blob/main/tools/convert_crowdhuman_to_coco.py). Then, you need to create a Exp file for your dataset. You can refer to the [CrowdHuman](https://github.com/ifzhang/ByteTrack/blob/main/exps/example/mot/yolox_x_ch.py) training Exp file. Don't forget to modify get_data_loader() and get_eval_loader in your Exp file. Finally, you can train bytetrack on your dataset by running:
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/train.py -f exps/example/mot/your_exp_file.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth
        ```
        
        
        ## Tracking
        
        * **Evaluation on MOT17 half val**
        
        Run ByteTrack:
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/track.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse
        ```
        You can get 76.6 MOTA using our pretrained model.
        
        Run other trackers:
        ```shell
        python3 tools/track_sort.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse
        python3 tools/track_deepsort.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse
        python3 tools/track_motdt.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse
        ```
        
        * **Test on MOT17**
        
        Run ByteTrack:
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/track.py -f exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar -b 1 -d 1 --fp16 --fuse
        python3 tools/interpolation.py
        ```
        Submit the txt files to [MOTChallenge](https://motchallenge.net/) website and you can get 79+ MOTA (For 80+ MOTA, you need to carefully tune the test image size and high score detection threshold of each sequence).
        
        * **Test on MOT20**
        
        We use the input size 1600 x 896 for MOT20-04, MOT20-07 and 1920 x 736 for MOT20-06, MOT20-08. You can edit it in [yolox_x_mix_mot20_ch.py](https://github.com/ifzhang/ByteTrack/blob/main/exps/example/mot/yolox_x_mix_mot20_ch.py)
        
        Run ByteTrack:
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/track.py -f exps/example/mot/yolox_x_mix_mot20_ch.py -c pretrained/bytetrack_x_mot20.pth.tar -b 1 -d 1 --fp16 --fuse --match_thresh 0.7 --mot20
        python3 tools/interpolation.py
        ```
        Submit the txt files to [MOTChallenge](https://motchallenge.net/) website and you can get 77+ MOTA (For higher MOTA, you need to carefully tune the test image size and high score detection threshold of each sequence).
        
        ## Applying BYTE to other trackers
        
        See [tutorials](https://github.com/ifzhang/ByteTrack/tree/main/tutorials).
        
        ## Combining BYTE with other detectors
        
        Suppose you have already got the detection results 'dets' (x1, y1, x2, y2, score) from other detectors, you can simply pass the detection results to BYTETracker (you need to first modify some post-processing code according to the format of your detection results in [byte_tracker.py](https://github.com/ifzhang/ByteTrack/blob/main/yolox/tracker/byte_tracker.py)):
        
        ```
        from yolox.tracker.byte_tracker import BYTETracker
        tracker = BYTETracker(args)
        for image in images:
           dets = detector(image)
           online_targets = tracker.update(dets, info_imgs, img_size)
        ```
        
        You can get the tracking results in each frame from 'online_targets'. You can refer to [mot_evaluators.py](https://github.com/ifzhang/ByteTrack/blob/main/yolox/evaluators/mot_evaluator.py) to pass the detection results to BYTETracker.
        
        ## Demo
        
        <img src="assets/palace_demo.gif" width="600"/>
        
        ```shell
        cd <ByteTrack_HOME>
        python3 tools/demo_track.py video -f exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar --fp16 --fuse --save_result
        ```
        
        ## Deploy
        
        1.  [ONNX export and ONNXRuntime](./deploy/ONNXRuntime)
        2.  [TensorRT in Python](./deploy/TensorRT/python)
        3.  [TensorRT in C++](./deploy/TensorRT/cpp)
        4.  [ncnn in C++](./deploy/ncnn/cpp)
        5.  [Deepstream](./deploy/DeepStream)
        
        ## Citation
        
        ```
        @article{zhang2022bytetrack,
          title={ByteTrack: Multi-Object Tracking by Associating Every Detection Box},
          author={Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},
          booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
          year={2022}
        }
        ```
        
        ## Acknowledgement
        
        A large part of the code is borrowed from [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX), [FairMOT](https://github.com/ifzhang/FairMOT), [TransTrack](https://github.com/PeizeSun/TransTrack) and [JDE-Cpp](https://github.com/samylee/Towards-Realtime-MOT-Cpp). Many thanks for their wonderful works.
        =======
        # football-server
        
        
        
        ## Getting started
        
        To make it easy for you to get started with GitLab, here's a list of recommended next steps.
        
        Already a pro? Just edit this README.md and make it your own. Want to make it easy? [Use the template at the bottom](#editing-this-readme)!
        
        ## Add your files
        
        - [ ] [Create](https://docs.gitlab.com/ee/user/project/repository/web_editor.html#create-a-file) or [upload](https://docs.gitlab.com/ee/user/project/repository/web_editor.html#upload-a-file) files
        - [ ] [Add files using the command line](https://docs.gitlab.com/ee/gitlab-basics/add-file.html#add-a-file-using-the-command-line) or push an existing Git repository with the following command:
        
        ```
        cd existing_repo
        git remote add origin https://gitlab.com/Menghang01/football-server.git
        git branch -M main
        git push -uf origin main
        ```
        
        ## Integrate with your tools
        
        - [ ] [Set up project integrations](https://gitlab.com/Menghang01/football-server/-/settings/integrations)
        
        ## Collaborate with your team
        
        - [ ] [Invite team members and collaborators](https://docs.gitlab.com/ee/user/project/members/)
        - [ ] [Create a new merge request](https://docs.gitlab.com/ee/user/project/merge_requests/creating_merge_requests.html)
        - [ ] [Automatically close issues from merge requests](https://docs.gitlab.com/ee/user/project/issues/managing_issues.html#closing-issues-automatically)
        - [ ] [Enable merge request approvals](https://docs.gitlab.com/ee/user/project/merge_requests/approvals/)
        - [ ] [Automatically merge when pipeline succeeds](https://docs.gitlab.com/ee/user/project/merge_requests/merge_when_pipeline_succeeds.html)
        
        ## Test and Deploy
        
        Use the built-in continuous integration in GitLab.
        
        - [ ] [Get started with GitLab CI/CD](https://docs.gitlab.com/ee/ci/quick_start/index.html)
        - [ ] [Analyze your code for known vulnerabilities with Static Application Security Testing(SAST)](https://docs.gitlab.com/ee/user/application_security/sast/)
        - [ ] [Deploy to Kubernetes, Amazon EC2, or Amazon ECS using Auto Deploy](https://docs.gitlab.com/ee/topics/autodevops/requirements.html)
        - [ ] [Use pull-based deployments for improved Kubernetes management](https://docs.gitlab.com/ee/user/clusters/agent/)
        - [ ] [Set up protected environments](https://docs.gitlab.com/ee/ci/environments/protected_environments.html)
        
        ***
        
        # Editing this README
        
        When you're ready to make this README your own, just edit this file and use the handy template below (or feel free to structure it however you want - this is just a starting point!). Thank you to [makeareadme.com](https://www.makeareadme.com/) for this template.
        
        ## Suggestions for a good README
        Every project is different, so consider which of these sections apply to yours. The sections used in the template are suggestions for most open source projects. Also keep in mind that while a README can be too long and detailed, too long is better than too short. If you think your README is too long, consider utilizing another form of documentation rather than cutting out information.
        
        ## Name
        Choose a self-explaining name for your project.
        
        ## Description
        Let people know what your project can do specifically. Provide context and add a link to any reference visitors might be unfamiliar with. A list of Features or a Background subsection can also be added here. If there are alternatives to your project, this is a good place to list differentiating factors.
        
        ## Badges
        On some READMEs, you may see small images that convey metadata, such as whether or not all the tests are passing for the project. You can use Shields to add some to your README. Many services also have instructions for adding a badge.
        
        ## Visuals
        Depending on what you are making, it can be a good idea to include screenshots or even a video (you'll frequently see GIFs rather than actual videos). Tools like ttygif can help, but check out Asciinema for a more sophisticated method.
        
        ## Installation
        Within a particular ecosystem, there may be a common way of installing things, such as using Yarn, NuGet, or Homebrew. However, consider the possibility that whoever is reading your README is a novice and would like more guidance. Listing specific steps helps remove ambiguity and gets people to using your project as quickly as possible. If it only runs in a specific context like a particular programming language version or operating system or has dependencies that have to be installed manually, also add a Requirements subsection.
        
        ## Usage
        Use examples liberally, and show the expected output if you can. It's helpful to have inline the smallest example of usage that you can demonstrate, while providing links to more sophisticated examples if they are too long to reasonably include in the README.
        
        ## Support
        Tell people where they can go to for help. It can be any combination of an issue tracker, a chat room, an email address, etc.
        
        ## Roadmap
        If you have ideas for releases in the future, it is a good idea to list them in the README.
        
        ## Contributing
        State if you are open to contributions and what your requirements are for accepting them.
        
        For people who want to make changes to your project, it's helpful to have some documentation on how to get started. Perhaps there is a script that they should run or some environment variables that they need to set. Make these steps explicit. These instructions could also be useful to your future self.
        
        You can also document commands to lint the code or run tests. These steps help to ensure high code quality and reduce the likelihood that the changes inadvertently break something. Having instructions for running tests is especially helpful if it requires external setup, such as starting a Selenium server for testing in a browser.
        
        ## Authors and acknowledgment
        Show your appreciation to those who have contributed to the project.
        
        ## License
        For open source projects, say how it is licensed.
        
        ## Project status
        If you have run out of energy or time for your project, put a note at the top of the README saying that development has slowed down or stopped completely. Someone may choose to fork your project or volunteer to step in as a maintainer or owner, allowing your project to keep going. You can also make an explicit request for maintainers.
        >>>>>>> b4efbb2629c3093b1d5584573d28f65a214a979c
        
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
